{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "mfccs = sklearn.preprocessing.scale(mfccs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cmap = plt.get_cmap('inferno')\n",
    "data=[]\n",
    "plt.figure(figsize=(10,10))\n",
    "genres = 'blues classical country disco hiphop jazz metal pop reggae rock'.split()\n",
    "for g in genres:\n",
    "    pathlib.Path(f'img_data/{g}').mkdir(parents=True, exist_ok=True)     \n",
    "    for filename in os.listdir(f'./genres/{g}'):\n",
    "        songname = f'./genres/{g}/{filename}'\n",
    "        y,sr = librosa.load(songname, mono=True, duration=5)\n",
    "        S=librosa.feature.mfcc(y=y,sr=sr,n_mfcc=50)\n",
    "        #data.append(y)\n",
    "        #D= np.abs(librosa.stft(y))**2\n",
    "        #S = librosa.feature.melspectrogram(S=D)\n",
    "        data.append(S)\n",
    "        #S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128,fmax=8000)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 50, 216)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1=np.array(data)\n",
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2= pd.read_csv('data.csv')\n",
    "data2.head()\n",
    "\n",
    "\n",
    "\n",
    "# Dropping unneccesary columns\n",
    "data2 = data2.drop(['filename'],axis=1)\n",
    "genres = 'blues classical country disco hiphop jazz metal pop reggae rock'\n",
    "genre=[x for x in genres.split(' ')]\n",
    "for i in range(len(genre)):\n",
    "    data2=data2.replace(genre[i],i)\n",
    "    \n",
    "data2['label'].values.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(data1,data2['label'].values , test_size=0.2)\n",
    "valX=testX[100:]\n",
    "valY=testY[100:]\n",
    "testX=testX[:100]\n",
    "testY=testY[:100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 50, 216)\n",
      "(800,)\n",
      "(100, 50, 216)\n",
      "(100,)\n",
      "(100, 50, 216)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "print (trainX.shape)\n",
    "print (trainY.shape)\n",
    "print (valX.shape)\n",
    "print (valY.shape)\n",
    "print (testX.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "batch_size=80\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(trainX), torch.from_numpy(trainY))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_data = TensorDataset(torch.from_numpy(valX), torch.from_numpy(valY))\n",
    "valid_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
    "test_data = TensorDataset(torch.from_numpy(testX), torch.from_numpy(testY))\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Administrator\\\\Desktop\\\\CS DATA PROJECT'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('data.csv')\n",
    "data.head()\n",
    "\n",
    "\n",
    "\n",
    "# Dropping unneccesary columns\n",
    "data = data.drop(['filename'],axis=1)\n",
    "genres = 'blues classical country disco hiphop jazz metal pop reggae rock'\n",
    "genre=[x for x in genres.split(' ')]\n",
    "for i in range(len(genre)):\n",
    "  data=data.replace(genre[i],i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mfcc1</th>\n",
       "      <th>mfcc2</th>\n",
       "      <th>mfcc3</th>\n",
       "      <th>mfcc4</th>\n",
       "      <th>mfcc5</th>\n",
       "      <th>mfcc6</th>\n",
       "      <th>mfcc7</th>\n",
       "      <th>mfcc8</th>\n",
       "      <th>mfcc9</th>\n",
       "      <th>mfcc10</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc12</th>\n",
       "      <th>mfcc13</th>\n",
       "      <th>mfcc14</th>\n",
       "      <th>mfcc15</th>\n",
       "      <th>mfcc16</th>\n",
       "      <th>mfcc17</th>\n",
       "      <th>mfcc18</th>\n",
       "      <th>mfcc19</th>\n",
       "      <th>mfcc20</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-113.596742</td>\n",
       "      <td>121.557302</td>\n",
       "      <td>-19.158825</td>\n",
       "      <td>42.351029</td>\n",
       "      <td>-6.376457</td>\n",
       "      <td>18.618875</td>\n",
       "      <td>-13.697911</td>\n",
       "      <td>15.34463</td>\n",
       "      <td>-12.285266</td>\n",
       "      <td>10.980491</td>\n",
       "      <td>...</td>\n",
       "      <td>8.810668</td>\n",
       "      <td>-3.667367</td>\n",
       "      <td>5.75169</td>\n",
       "      <td>-5.162761</td>\n",
       "      <td>0.750947</td>\n",
       "      <td>-1.691937</td>\n",
       "      <td>-0.409954</td>\n",
       "      <td>-2.300208</td>\n",
       "      <td>1.219928</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        mfcc1       mfcc2      mfcc3      mfcc4     mfcc5      mfcc6  \\\n",
       "0 -113.596742  121.557302 -19.158825  42.351029 -6.376457  18.618875   \n",
       "\n",
       "       mfcc7     mfcc8      mfcc9     mfcc10  ...    mfcc12    mfcc13  \\\n",
       "0 -13.697911  15.34463 -12.285266  10.980491  ...  8.810668 -3.667367   \n",
       "\n",
       "    mfcc14    mfcc15    mfcc16    mfcc17    mfcc18    mfcc19    mfcc20  label  \n",
       "0  5.75169 -5.162761  0.750947 -1.691937 -0.409954 -2.300208  1.219928      0  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=data.drop(['chroma_stft','rmse','spectral_centroid','spectral_bandwidth','rolloff','zero_crossing_rate'],axis=1)\n",
    "data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data2=data.sample(n=800, random_state=1)\n",
    "for i in range(len(data)-1,-1,-1):\n",
    "  if data.values.tolist()[i] in data2.values.tolist():\n",
    "    data=data.drop([i])\n",
    "n_classes = 10\n",
    "data1=data[list(data.keys())[:-1]].values.reshape(-1,len(list(data.keys()))-1)\n",
    "\n",
    "\n",
    "valX, testX, valY, testY = train_test_split(data[list(data.keys())[:-1]].values.reshape(-1,len(list(data.keys()))-1),data['label'].values , test_size=0.5)\n",
    "trainX,trainY=data2[list(data.keys())[:-1]].values.reshape(-1,len(list(data.keys()))-1),data2['label'].values\n",
    "#trainX, trainY, valX, valY, testX = data1[:800],data['label'].values[:800],data1[900:],data['label'].values[900:],data1[800:900]\n",
    "#df_test_set= data[list(data.keys())].values[800:900].reshape(-1,len(list(data.keys())))\n",
    "\n",
    "#print (df_test_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 20)\n",
      "(800,)\n",
      "(98, 20)\n",
      "(98,)\n",
      "(98, 20)\n",
      "(98,)\n"
     ]
    }
   ],
   "source": [
    "print (trainX.shape)\n",
    "print (trainY.shape)\n",
    "print (valX.shape)\n",
    "print (valY.shape)\n",
    "print (testX.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:USE CUDA=True\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "handler=logging.basicConfig(level=logging.INFO)\n",
    "lgr = logging.getLogger(__name__)\n",
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# move tensors to GPU if CUDA is available\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "lgr.info(\"USE CUDA=\" + str (use_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XnumpyToTensor(x_data_np):\n",
    "    x_data_np = np.array(x_data_np, dtype=np.float32)        \n",
    "    print(x_data_np.shape)\n",
    "    print(type(x_data_np))\n",
    "\n",
    "    if use_cuda:\n",
    "        lgr.info (\"Using the GPU\")    \n",
    "        X_tensor = Variable(torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n",
    "    else:\n",
    "        lgr.info (\"Using the CPU\")\n",
    "        X_tensor = Variable(torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n",
    "    \n",
    "    print(type(X_tensor.data)) # should be 'torch.cuda.FloatTensor'            \n",
    "    print((X_tensor.data.shape)) # torch.Size([108405, 29])\n",
    "    return X_tensor\n",
    "\n",
    "\n",
    "# Convert the np arrays into the correct dimention and type\n",
    "# Note that BCEloss requires Float in X as well as in y\n",
    "def YnumpyToTensor(y_data_np):    \n",
    "    y_data_np=y_data_np.reshape((y_data_np.shape[0],1)) # Must be reshaped for PyTorch!\n",
    "    print(y_data_np.shape)\n",
    "    print(type(y_data_np))\n",
    "\n",
    "    if use_cuda:\n",
    "        lgr.info (\"Using the GPU\")            \n",
    "    #     Y = Variable(torch.from_numpy(y_data_np).type(torch.LongTensor).cuda())\n",
    "        Y_tensor = Variable(torch.from_numpy(y_data_np)).type(torch.LongTensor).cuda()  # BCEloss requires Float        \n",
    "    else:\n",
    "        lgr.info (\"Using the CPU\")        \n",
    "    #     Y = Variable(torch.squeeze (torch.from_numpy(y_data_np).type(torch.LongTensor)))  #         \n",
    "        Y_tensor = Variable(torch.from_numpy(y_data_np)).type(torch.LongTensor)  # BCEloss requires Float        \n",
    "\n",
    "    print(type(Y_tensor.data)) # should be 'torch.cuda.FloatTensor'\n",
    "    print(y_data_np.shape)\n",
    "    print(type(y_data_np))    \n",
    "    return Y_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 50, 216)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([800, 50, 216])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Net2(\n",
      "  (c1): Sequential(\n",
      "    (0): Conv1d(50, 100, kernel_size=(2,), stride=(1,), padding=(1,))\n",
      "    (1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "    (2): Dropout(p=0.25)\n",
      "    (3): LeakyReLU(negative_slope=0.1)\n",
      "    (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (c2): Sequential(\n",
      "    (0): Conv1d(100, 200, kernel_size=(2,), stride=(1,), padding=(1,), dilation=(4,))\n",
      "    (1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "    (2): Dropout(p=0.25)\n",
      "    (3): LeakyReLU(negative_slope=0.1)\n",
      "    (4): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (out): Sequential(\n",
      "    (0): Linear(in_features=520, out_features=10, bias=True)\n",
      "  )\n",
      "  (sof): Sigmoid()\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1 input:  torch.Size([800, 50, 216])\n",
      "c1 output:  torch.Size([800, 100, 108])\n",
      "c2 input:  torch.Size([800, 100, 108])\n",
      "c2 output:  torch.Size([800, 200, 53])\n",
      "l2 input:  torch.Size([800, 10600])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [800 x 10600], m2: [520 x 10] at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1533090623466\\work\\aten\\src\\thc\\generic/THCTensorMathBlas.cu:249",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-14a6b6d7beef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# very important !!!\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[0mlgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tensor_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'(b.size():'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# torch.Size([108405, 928])\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-14a6b6d7beef>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[1;31m#         x=self.l2(x)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'l2 out: '\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m         \u001b[1;31m# fused op is marginally faster\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1024\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1026\u001b[0m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch, m1: [800 x 10600], m2: [520 x 10] at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1533090623466\\work\\aten\\src\\thc\\generic/THCTensorMathBlas.cu:249"
     ]
    }
   ],
   "source": [
    "# use_cuda=False\n",
    "X_tensor_train= XnumpyToTensor(trainX) # default order is NBC for a 3d tensor, but we have a 2d tensor\n",
    "X_shape=X_tensor_train.data.size()\n",
    "\n",
    "# Dimensions\n",
    "# Number of features for the input layer\n",
    "N_FEATURES=trainX.shape[1]\n",
    "# Number of rows\n",
    "NUM_ROWS_TRAINNING=trainX.shape[0]\n",
    "# this number has no meaning except for being divisable by 2\n",
    "N_MULT_FACTOR=4 # min should be 4\n",
    "# Size of first linear layer\n",
    "N_HIDDEN=N_FEATURES*2\n",
    "# CNN kernel size\n",
    "N_CNN_KERNEL=2\n",
    "MAX_POOL_KERNEL=4\n",
    "\n",
    "DEBUG_ON=False\n",
    "\n",
    "def debug(x):\n",
    "    if DEBUG_ON:\n",
    "        print ('(x.size():' + str (x.size()))\n",
    "    \n",
    "class Net2(nn.Module):    \n",
    "    def __init__(self, n_feature, n_hidden, n_output, n_cnn_kernel, n_mult_factor=N_MULT_FACTOR,n=8):\n",
    "        super(Net2, self).__init__()\n",
    "        self.n_feature=n_feature\n",
    "        self.n_hidden=int(n_hidden)\n",
    "        self.n_output= n_output \n",
    "        self.n_cnn_kernel=n_cnn_kernel\n",
    "        self.n_mult_factor=n_mult_factor\n",
    "        #self.n_l2_hidden=self.n_hidden * (self.n_mult_factor - self.n_cnn_kernel + 3)\n",
    "        self.n_l2_hidden=10400\n",
    "#         self.n_out_hidden=int (self.n_l2_hidden/2)\n",
    "                        \n",
    "     \n",
    "        self.c1= nn.Sequential(            \n",
    "            torch.nn.Conv1d(self.n_feature, self.n_hidden, \n",
    "                            kernel_size=(self.n_cnn_kernel,), stride=(1,), padding=(1,),dilation=4),\n",
    "            torch.nn.AvgPool1d(self.n_cnn_kernel),\n",
    "            torch.nn.Dropout(p=1 -.75),            \n",
    "            torch.nn.LeakyReLU (0.1),\n",
    "            torch.nn.BatchNorm1d(self.n_hidden, eps=1e-05, momentum=0.1, affine=True)        \n",
    "        )         \n",
    "        self.c2= nn.Sequential(            \n",
    "            torch.nn.Conv1d(self.n_hidden, 2*self.n_hidden, \n",
    "                            kernel_size=(self.n_cnn_kernel,), stride=(1,),padding=(1,), dilation=4),\n",
    "            torch.nn.AvgPool1d(self.n_cnn_kernel),\n",
    "            torch.nn.Dropout(p=1 -.75),            \n",
    "            torch.nn.LeakyReLU (0.1),\n",
    "            torch.nn.BatchNorm1d(2*self.n_hidden, eps=1e-05, momentum=0.1, affine=True)        \n",
    "        )    \n",
    "        self.out = nn.Sequential(\n",
    "            torch.nn.Linear(self.n_l2_hidden,\n",
    "                            self.n_output),  \n",
    "        )                \n",
    "        self.sof=nn.Softmax()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #debug(x)\n",
    "        #print('l1 in: ',x.shape)\n",
    "        varSize=x.data.shape[0] # must be calculated here in forward() since its is a dynamic size        \n",
    "        #x=self.l1(x) \n",
    "        #print('l1 out: ',x.shape)\n",
    "        #debug(x)\n",
    "        # for CNN        \n",
    "        #x = x.view(varSize,1,-1)\n",
    "        #print('c1 input: ',x.shape)\n",
    "        debug(x)\n",
    "        x=self.c1(x)\n",
    "        print('c1 output: ',x.shape)\n",
    "        debug(x)\n",
    "        #x = x.view(varSize,self.n_feature,-1)\n",
    "        print('c2 input: ',x.shape)\n",
    "        x=self.c2(x)\n",
    "        print('c2 output: ',x.shape)\n",
    "        #debug(x)\n",
    " \n",
    "        # for Linear layer\n",
    "        #x = x.view(varSize, self.n_hidden * (self.n_mult_factor -self.n_cnn_kernel + 3))\n",
    "        x = x.view(varSize, -1)\n",
    "        print('l2 input: ',x.shape)\n",
    "        debug(x)\n",
    "#         x=self.l2(x) \n",
    "\n",
    "        x=self.out(x)   \n",
    "        print('l2 out: ',x.shape)\n",
    "        debug(x)\n",
    "        x=self.sof(x)\n",
    "        print('sof out: ',x.shape)\n",
    "        return x\n",
    "    \n",
    "\n",
    "net = Net2(n_feature=N_FEATURES, n_hidden=N_HIDDEN, n_output=10, n_cnn_kernel=N_CNN_KERNEL)   # define the network    \n",
    "if use_cuda:\n",
    "    net=net.cuda() # very important !!!\n",
    "lgr.info(net)\n",
    "b = net(X_tensor_train)\n",
    "print ('(b.size():' + str (b.size())) # torch.Size([108405, 928])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 50, 216)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([800, 50, 216])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Net2(\n",
      "  (c1): Sequential(\n",
      "    (0): Conv1d(50, 100, kernel_size=(2,), stride=(1,), padding=(1,), dilation=(4,))\n",
      "    (1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "    (2): Dropout(p=0.25)\n",
      "    (3): LeakyReLU(negative_slope=0.1)\n",
      "    (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (c2): Sequential(\n",
      "    (0): Conv1d(100, 200, kernel_size=(2,), stride=(1,), padding=(1,), dilation=(4,))\n",
      "    (1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
      "    (2): Dropout(p=0.25)\n",
      "    (3): LeakyReLU(negative_slope=0.1)\n",
      "    (4): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (out): Sequential(\n",
      "    (0): Linear(in_features=10400, out_features=10, bias=True)\n",
      "  )\n",
      "  (sof): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# use_cuda=False\n",
    "X_tensor_train= XnumpyToTensor(trainX) # default order is NBC for a 3d tensor, but we have a 2d tensor\n",
    "X_shape=X_tensor_train.data.size()\n",
    "\n",
    "# Dimensions\n",
    "# Number of features for the input layer\n",
    "N_FEATURES=trainX.shape[1]\n",
    "# Number of rows\n",
    "NUM_ROWS_TRAINNING=trainX.shape[0]\n",
    "# this number has no meaning except for being divisable by 2\n",
    "N_MULT_FACTOR=4 # min should be 4\n",
    "# Size of first linear layer\n",
    "N_HIDDEN=N_FEATURES*2\n",
    "# CNN kernel size\n",
    "N_CNN_KERNEL=2\n",
    "MAX_POOL_KERNEL=4\n",
    "\n",
    "DEBUG_ON=False\n",
    "\n",
    "def debug(x):\n",
    "    if DEBUG_ON:\n",
    "        print ('(x.size():' + str (x.size()))\n",
    "    \n",
    "class Net2(nn.Module):    \n",
    "    def __init__(self, n_feature, n_hidden, n_output, n_cnn_kernel, n_mult_factor=N_MULT_FACTOR,n=8):\n",
    "        super(Net2, self).__init__()\n",
    "        self.n_feature=n_feature\n",
    "        self.n_hidden=int(n_hidden)\n",
    "        self.n_output= n_output \n",
    "        self.n_cnn_kernel=n_cnn_kernel\n",
    "        self.n_mult_factor=n_mult_factor\n",
    "        #self.n_l2_hidden=self.n_hidden * (self.n_mult_factor - self.n_cnn_kernel + 3)\n",
    "        self.n_l2_hidden=10400\n",
    "#         self.n_out_hidden=int (self.n_l2_hidden/2)\n",
    "                        \n",
    "     \n",
    "        self.c1= nn.Sequential(            \n",
    "            torch.nn.Conv1d(self.n_feature, self.n_hidden, \n",
    "                            kernel_size=(self.n_cnn_kernel,), stride=(1,), padding=(1,),dilation=4),\n",
    "            torch.nn.AvgPool1d(self.n_cnn_kernel),\n",
    "            torch.nn.Dropout(p=1 -.75),            \n",
    "            torch.nn.LeakyReLU (0.1),\n",
    "            torch.nn.BatchNorm1d(self.n_hidden, eps=1e-05, momentum=0.1, affine=True)        \n",
    "        )         \n",
    "        self.c2= nn.Sequential(            \n",
    "            torch.nn.Conv1d(self.n_hidden, 2*self.n_hidden, \n",
    "                            kernel_size=(self.n_cnn_kernel,), stride=(1,),padding=(1,), dilation=4),\n",
    "            torch.nn.AvgPool1d(self.n_cnn_kernel),\n",
    "            torch.nn.Dropout(p=1 -.75),            \n",
    "            torch.nn.LeakyReLU (0.1),\n",
    "            torch.nn.BatchNorm1d(2*self.n_hidden, eps=1e-05, momentum=0.1, affine=True)        \n",
    "        )    \n",
    "        self.out = nn.Sequential(\n",
    "            torch.nn.Linear(self.n_l2_hidden,\n",
    "                            self.n_output),  \n",
    "        )                \n",
    "        self.sof=nn.Softmax()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #debug(x)\n",
    "        #print('l1 in: ',x.shape)\n",
    "        varSize=x.data.shape[0] # must be calculated here in forward() since its is a dynamic size        \n",
    "        #x=self.l1(x) \n",
    "        #print('l1 out: ',x.shape)\n",
    "        #debug(x)\n",
    "        # for CNN        \n",
    "        #x = x.view(varSize,1,-1)\n",
    "        #print('c1 input: ',x.shape)\n",
    "        debug(x)\n",
    "        x=self.c1(x)\n",
    "        #print('c1 output: ',x.shape)\n",
    "        debug(x)\n",
    "        #x = x.view(varSize,self.n_feature,-1)\n",
    "        #print('c2 input: ',x.shape)\n",
    "        x=self.c2(x)\n",
    "        #print('c2 output: ',x.shape)\n",
    "        #debug(x)\n",
    " \n",
    "        # for Linear layer\n",
    "        #x = x.view(varSize, self.n_hidden * (self.n_mult_factor -self.n_cnn_kernel + 3))\n",
    "        x = x.view(varSize, -1)\n",
    "        #print('l2 input: ',x.shape)\n",
    "        debug(x)\n",
    "#         x=self.l2(x) \n",
    "\n",
    "        x=self.out(x)   \n",
    "        #print('l2 out: ',x.shape)\n",
    "        debug(x)\n",
    "        x=self.sof(x)\n",
    "        #print('sof out: ',x.shape)\n",
    "        return x\n",
    "    \n",
    "\n",
    "net = Net2(n_feature=N_FEATURES, n_hidden=N_HIDDEN, n_output=10, n_cnn_kernel=N_CNN_KERNEL)   # define the network    \n",
    "if use_cuda:\n",
    "    net=net.cuda() # very important !!!\n",
    "lgr.info(net)\n",
    "#b = net(X_tensor_train)\n",
    "#print ('(b.size():' + str (b.size())) # torch.Size([108405, 928])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 50, 216)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the GPU\n",
      "INFO:__main__:Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    lr: 5e-06\n",
      "    weight_decay: 0\n",
      ")\n",
      "INFO:__main__:CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0000025)\n",
    "if use_cuda:\n",
    "    lgr.info (\"Using the GPU\")    \n",
    "    net.cuda()\n",
    "    loss_func.cuda()\n",
    "\n",
    "lgr.info (optimizer)\n",
    "lgr.info (loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "batch_size=80\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(trainX), torch.from_numpy(trainY))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_data = TensorDataset(torch.from_numpy(valX), torch.from_numpy(valY))\n",
    "valid_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
    "test_data = TensorDataset(torch.from_numpy(testX), torch.from_numpy(testY))\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 50, 216)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using the GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([800, 50, 216])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:90: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/600... Step: 100... Loss: 1.536524... Val Loss: 1.956185\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-ab74069f0b6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# clear gradients for next train\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m         \u001b[1;31m# backpropagation, compute gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m        \u001b[1;31m# apply gradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m#train_loss+=cost.item()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "start_time = time.time()    \n",
    "epochs=600\n",
    "all_losses = []\n",
    "#batch_size=80\n",
    "X_tensor_train= XnumpyToTensor(trainX)\n",
    "#Y_tensor_train= YnumpyToTensor(trainY)\n",
    "counter=0\n",
    "#print(type(X_tensor_train.data), type(Y_tensor_train.data)) # should be 'torch.cuda.FloatTensor'\n",
    "net.train()\n",
    "# From here onwards, we must only use PyTorch Tensors\n",
    "for step in range(epochs):\n",
    "  train_loss=0.0\n",
    "\n",
    "  for X_t_train, Y_t_train in train_loader:   \n",
    "    counter+=1\n",
    "    if (train_on_gpu):\n",
    "        X_t_train,Y_t_train=X_t_train.to('cuda'),Y_t_train.to('cuda')\n",
    "    \n",
    "    out = net(X_t_train.float())                 # input x and predict based on x\n",
    "    loss = loss_func(out, Y_t_train.squeeze_())     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "    #loss = loss_func(out, Y_t_train.to(dtype=torch.float))   \n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "\n",
    "    #train_loss+=cost.item()\n",
    "\n",
    "    if counter % 100 == 0:\n",
    "        # Get validation loss\n",
    "\n",
    "        val_losses = []\n",
    "        net.eval()\n",
    "        for inputs, labels in valid_loader:\n",
    "            \n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            output= net(inputs.float())\n",
    "            val_loss = loss_func(output, labels)\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "        \n",
    "        print(\"Epoch: {}/{}...\".format(step+1, epochs),\n",
    "              \"Step: {}...\".format(counter),\n",
    "              \"Loss: {:.6f}...\".format(loss.item()),\n",
    "              \"Val Loss: {:.6f}\".format(np.mean(val_losses)))      \n",
    "\n",
    "        #prediction = out.data # probabilities             \n",
    "        #_,pred_y = torch.max(out.data, 1)\n",
    "        #pred_y=pred_y.numpy()\n",
    "        #target_y = Y_t_train.cpu().data.numpy()\n",
    "        #target_y=torch.from_numpy(target_y)\n",
    "       #print('pred_y: ',pred_y)\n",
    "        #print('target: ',target_y)      \n",
    "        #accuracy=np.sum(pred_y==target_y)/float(len(target_y))\n",
    "\n",
    "        #print('Accuracy: ',accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel_launcher.py:90: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.297\n",
      "Test accuracy: 0.100\n"
     ]
    }
   ],
   "source": [
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output= net(inputs.float())\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = loss_func(output, labels)\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    _,pred = torch.max(torch.round(output.squeeze()).data,1)  # rounds to the nearest integer\n",
    "    #print('pred: ',pred)\n",
    "    #print('labels: ',labels.shape)\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 7, 1, 5, 2, 4, 6, 8, 5, 9, 2, 2, 6, 6, 9, 6, 5, 3, 0, 3], device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2222222222222222"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
