{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from PIL import Image\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "\n",
    "import librosa\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pathlib\n",
    "import csv\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract mfcc features from GTZAN datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.get_cmap('inferno')\n",
    "data=[]\n",
    "plt.figure(figsize=(10,10))\n",
    "genres = 'blues classical country disco hiphop jazz metal pop reggae rock'.split()\n",
    "for g in genres:\n",
    "    pathlib.Path(f'img_data/{g}').mkdir(parents=True, exist_ok=True)     \n",
    "    for filename in os.listdir(f'./genres/{g}'):\n",
    "        songname = f'./genres/{g}/{filename}'\n",
    "        y,sr = librosa.load(songname, mono=True, duration=5)\n",
    "        S=librosa.feature.mfcc(y=y,sr=sr,n_mfcc=50)\n",
    "        #S = scale(S, axis=1)\n",
    "        #data.append(y)\n",
    "        #D= np.abs(librosa.stft(y))**2\n",
    "        #S = librosa.feature.melspectrogram(S=D)\n",
    "        data.append(S)\n",
    "        #S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128,fmax=8000)\n",
    "data1=np.array(data)\n",
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2= pd.read_csv('data.csv')\n",
    "data2.head()\n",
    "# Dropping unneccesary columns\n",
    "data2 = data2.drop(['filename'],axis=1)\n",
    "genres = 'blues classical country disco hiphop jazz metal pop reggae rock'\n",
    "genre=[x for x in genres.split(' ')]\n",
    "for i in range(len(genre)):\n",
    "    data2=data2.replace(genre[i],i)\n",
    "    \n",
    "data2['label'].values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Set, Testing Set, Validing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(data1,data2['label'].values , test_size=0.2)\n",
    "valX=testX[100:]\n",
    "valY=testY[100:]\n",
    "testX=testX[:100]\n",
    "testY=testY[:100]\n",
    "print (trainX.shape)\n",
    "print (trainY.shape)\n",
    "print (valX.shape)\n",
    "print (valY.shape)\n",
    "print (testX.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "batch_size=20\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(trainX), torch.from_numpy(trainY))\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_data = TensorDataset(torch.from_numpy(valX), torch.from_numpy(valY))\n",
    "valid_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
    "test_data = TensorDataset(torch.from_numpy(testX), torch.from_numpy(testY))\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "handler=logging.basicConfig(level=logging.INFO)\n",
    "lgr = logging.getLogger(__name__)\n",
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# move tensors to GPU if CUDA is available\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "lgr.info(\"USE CUDA=\" + str (use_cuda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XnumpyToTensor(x_data_np):\n",
    "    x_data_np = np.array(x_data_np, dtype=np.float32)        \n",
    "    print(x_data_np.shape)\n",
    "    print(type(x_data_np))\n",
    "\n",
    "    if use_cuda:\n",
    "        lgr.info (\"Using the GPU\")    \n",
    "        X_tensor = Variable(torch.from_numpy(x_data_np).cuda()) # Note the conversion for pytorch    \n",
    "    else:\n",
    "        lgr.info (\"Using the CPU\")\n",
    "        X_tensor = Variable(torch.from_numpy(x_data_np)) # Note the conversion for pytorch\n",
    "    \n",
    "    print(type(X_tensor.data)) # should be 'torch.cuda.FloatTensor'            \n",
    "    print((X_tensor.data.shape)) # torch.Size([108405, 29])\n",
    "    return X_tensor\n",
    "\n",
    "\n",
    "# Convert the np arrays into the correct dimention and type\n",
    "# Note that BCEloss requires Float in X as well as in y\n",
    "def YnumpyToTensor(y_data_np):    \n",
    "    y_data_np=y_data_np.reshape((y_data_np.shape[0],1)) # Must be reshaped for PyTorch!\n",
    "    print(y_data_np.shape)\n",
    "    print(type(y_data_np))\n",
    "\n",
    "    if use_cuda:\n",
    "        lgr.info (\"Using the GPU\")            \n",
    "    #     Y = Variable(torch.from_numpy(y_data_np).type(torch.LongTensor).cuda())\n",
    "        Y_tensor = Variable(torch.from_numpy(y_data_np)).type(torch.LongTensor).cuda()  # BCEloss requires Float        \n",
    "    else:\n",
    "        lgr.info (\"Using the CPU\")        \n",
    "    #     Y = Variable(torch.squeeze (torch.from_numpy(y_data_np).type(torch.LongTensor)))  #         \n",
    "        Y_tensor = Variable(torch.from_numpy(y_data_np)).type(torch.LongTensor)  # BCEloss requires Float        \n",
    "\n",
    "    print(type(Y_tensor.data)) # should be 'torch.cuda.FloatTensor'\n",
    "    print(y_data_np.shape)\n",
    "    print(type(y_data_np))    \n",
    "    return Y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use_cuda=False\n",
    "X_tensor_train= XnumpyToTensor(trainX) # default order is NBC for a 3d tensor, but we have a 2d tensor\n",
    "X_shape=X_tensor_train.data.size()\n",
    "\n",
    "# Dimensions\n",
    "# Number of features for the input layer\n",
    "N_FEATURES=trainX.shape[1]\n",
    "# Number of rows\n",
    "NUM_ROWS_TRAINNING=trainX.shape[0]\n",
    "# this number has no meaning except for being divisable by 2\n",
    "N_MULT_FACTOR=4 # min should be 4\n",
    "# Size of first linear layer\n",
    "N_HIDDEN=N_FEATURES*2\n",
    "# CNN kernel size\n",
    "N_CNN_KERNEL=2\n",
    "MAX_POOL_KERNEL=4\n",
    "\n",
    "DEBUG_ON=False\n",
    "\n",
    "def debug(x):\n",
    "    if DEBUG_ON:\n",
    "        print ('(x.size():' + str (x.size()))\n",
    "    \n",
    "class Net2(nn.Module):    \n",
    "    def __init__(self, n_feature, n_hidden, n_output, n_cnn_kernel, n_mult_factor=N_MULT_FACTOR,n=8):\n",
    "        super(Net2, self).__init__()\n",
    "        self.n_feature=n_feature\n",
    "        self.n_hidden=int(n_hidden)\n",
    "        self.n_output= n_output \n",
    "        self.n_cnn_kernel=n_cnn_kernel\n",
    "        self.n_mult_factor=n_mult_factor\n",
    "        #self.n_l2_hidden=self.n_hidden * (self.n_mult_factor - self.n_cnn_kernel + 3)\n",
    "        self.n_l2_hidden=8320\n",
    "#         self.n_out_hidden=int (self.n_l2_hidden/2)\n",
    "                        \n",
    "     \n",
    "        self.c1= nn.Sequential(            \n",
    "            torch.nn.Conv1d(self.n_feature, self.n_hidden, \n",
    "                            kernel_size=(self.n_cnn_kernel,), stride=(1,), padding=(1,),dilation=4),\n",
    "            torch.nn.AvgPool1d(self.n_cnn_kernel),\n",
    "            torch.nn.Dropout(p=1 -.75),            \n",
    "            torch.nn.LeakyReLU (0.1),\n",
    "            torch.nn.BatchNorm1d(self.n_hidden, eps=1e-05, momentum=0.1, affine=True)        \n",
    "        )         \n",
    "        self.c2= nn.Sequential(            \n",
    "            torch.nn.Conv1d(self.n_hidden, 2*self.n_hidden, \n",
    "                            kernel_size=(self.n_cnn_kernel,), stride=(1,),padding=(1,), dilation=4),\n",
    "            torch.nn.AvgPool1d(self.n_cnn_kernel),\n",
    "            torch.nn.Dropout(p=1 -.75),            \n",
    "            torch.nn.LeakyReLU (0.1),\n",
    "            torch.nn.BatchNorm1d(2*self.n_hidden, eps=1e-05, momentum=0.1, affine=True)        \n",
    "        )    \n",
    "        self.out = nn.Sequential(\n",
    "            torch.nn.Linear(self.n_l2_hidden,\n",
    "                            self.n_output),  \n",
    "        )                \n",
    "        self.sof=nn.Softmax()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #debug(x)\n",
    "        #print('l1 in: ',x.shape)\n",
    "        varSize=x.data.shape[0] # must be calculated here in forward() since its is a dynamic size        \n",
    "        #x=self.l1(x) \n",
    "        #print('l1 out: ',x.shape)\n",
    "        #debug(x)\n",
    "        # for CNN        \n",
    "        #x = x.view(varSize,1,-1)\n",
    "        #print('c1 input: ',x.shape)\n",
    "        debug(x)\n",
    "        x=self.c1(x)\n",
    "        #print('c1 output: ',x.shape)\n",
    "        debug(x)\n",
    "        #x = x.view(varSize,self.n_feature,-1)\n",
    "        #print('c2 input: ',x.shape)\n",
    "        x=self.c2(x)\n",
    "        #print('c2 output: ',x.shape)\n",
    "        #debug(x)\n",
    "        # for Linear layer\n",
    "        #x = x.view(varSize, self.n_hidden * (self.n_mult_factor -self.n_cnn_kernel + 3))\n",
    "        x = x.view(varSize, -1)\n",
    "        #print('l2 input: ',x.shape)\n",
    "        debug(x)\n",
    "        x=self.out(x)   \n",
    "        #print('l2 out: ',x.shape)\n",
    "        debug(x)\n",
    "        x=self.sof(x)\n",
    "        #print('sof out: ',x.shape)\n",
    "        return x\n",
    "    \n",
    "\n",
    "net = Net2(n_feature=N_FEATURES, n_hidden=N_HIDDEN, n_output=10, n_cnn_kernel=N_CNN_KERNEL)   # define the network    \n",
    "if use_cuda:\n",
    "    net=net.cuda() \n",
    "lgr.info(net)\n",
    "#b = net(X_tensor_train)\n",
    "#print ('(b.size():' + str (b.size())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "if use_cuda:\n",
    "    lgr.info (\"Using the GPU\")    \n",
    "    net.cuda()\n",
    "    loss_func.cuda()\n",
    "\n",
    "lgr.info (optimizer)\n",
    "lgr.info (loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "start_time = time.time()    \n",
    "epochs=1500\n",
    "all_losses = []\n",
    "#batch_size=80\n",
    "X_tensor_train= XnumpyToTensor(trainX)\n",
    "#Y_tensor_train= YnumpyToTensor(trainY)\n",
    "counter=0\n",
    "#print(type(X_tensor_train.data), type(Y_tensor_train.data)) # should be 'torch.cuda.FloatTensor'\n",
    "net.train()\n",
    "# From here onwards, we must only use PyTorch Tensors\n",
    "for step in range(epochs):\n",
    "  train_loss=0.0\n",
    "\n",
    "  for X_t_train, Y_t_train in train_loader:   \n",
    "    counter+=1\n",
    "    if (train_on_gpu):\n",
    "        X_t_train,Y_t_train=X_t_train.to('cuda'),Y_t_train.to('cuda')\n",
    "    \n",
    "    out = net(X_t_train.float())                 # input x and predict based on x\n",
    "    loss = loss_func(out, Y_t_train.squeeze_())     # must be (1. nn output, 2. target), the target label is NOT one-hotted\n",
    "    #loss = loss_func(out, Y_t_train.to(dtype=torch.float))   \n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    loss.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients\n",
    "\n",
    "    #train_loss+=cost.item()\n",
    "\n",
    "    if counter % 100 == 0:\n",
    "        # Get validation loss\n",
    "\n",
    "        val_losses = []\n",
    "        net.eval()\n",
    "        for inputs, labels in valid_loader:\n",
    "            \n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "\n",
    "\n",
    "            if(train_on_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "            output= net(inputs.float())\n",
    "            val_loss = loss_func(output, labels)\n",
    "\n",
    "            val_losses.append(val_loss.item())\n",
    "\n",
    "        \n",
    "        print(\"Epoch: {}/{}...\".format(step+1, epochs),\n",
    "              \"Step: {}...\".format(counter),\n",
    "              \"Loss: {:.6f}...\".format(loss.item()),\n",
    "              \"Val Loss: {:.6f}\".format(np.mean(val_losses)))      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output= net(inputs.float())\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = loss_func(output, labels)\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    _,pred = torch.max(torch.round(output.squeeze()).data,1)  # rounds to the nearest integer\n",
    "    #print('pred: ',pred)\n",
    "    #print('labels: ',labels.shape)\n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
